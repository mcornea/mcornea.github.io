<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Network attached storage infrastructure &middot; remote-lab.net
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>a place to learn systems and networks administration</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/archive/">Archive</a>
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/docs/">Docs</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    

    <a class="sidebar-nav-item" href="https://compute.remote-lab.net">Compute</a>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2017. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">remote-lab.net</a>
            <small>learn by doing</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Network attached storage infrastructure</h1>
  <span class="post-date">07 Jul 2013</span>
  <p>Today we all need storage space and we get it in all kind of flavors: local disks, file servers on the local network, cloud storage, etc. In this post I’ll show you how you can quickly create a basic storage infrastructure that you can use for your data storage.</p>

<hr />

<p><a href="http://remote-lab.net:4000/public/images/storage.png"><img src="http://remote-lab.net:4000/public/images/storage.png" alt="storage" width="614" height="372" class="aligncenter size-full wp-image-198" /></a></p>

<p>As you may see in the diagram the topology consists of 2 clients, 2 switches and 2 storage servers. All the components in the diagram are KVM virtual machines running on a single host. You can easily create this setup on your own workstation.   Clients need to be able to access the files on the storage servers as they would on their local drives. In order to do that we can export file system paths on the storage servers by protocols such as NFS or SMB/CIFS. NFS is my favorite one so I’ll show how you can use it in this example. The clients will be basic Debian VMs running the NFS clients.</p>

<p>Hardware always fails so in order for our system to be available we need to have redundant components. The first storage server will act as the primary server, all the clients will access directly. Data on the primary server will be replicated on the second one which will act as a slave (standby). For the storage file systems we will use ZFS which is both a file system and logical volume manager. ZFS ensures data integrity checks and automatic repairs, it provides various software RAID levels and many other great features such as snapshots, compression, deduplication and replication. In addition to this ZFS is a 128-bit file system providing lots and lots of storage space. Unfortunately ZFS is licensed under Suns CDDL which is not compatible with the Linux kernel license: GPLv2. Work is currently being done for a port of ZFS for the Linux kernel ( ZFS on Linux ). In our topology the storage servers will run zfs on FreeBSD 9.1.</p>

<p>The network infrastructure is very important when we need redundant systems. Each of the storage servers will have 2 NICs grouped in a bridge connected to 2 different switches. In order to prevent bridge loops we need a loop prevention mechanism such as STP. In our environment we will use STP (802.1d) but be aware that in a production network you need other flavors of STP for reduced convergence time. The switches will run Debian as OS and Open vSwitch for the virtual switch.</p>

<p>We can now proceed and start creating our infrastructure. I will be running Debian jessie which is the current testing release on both the host machine and the virtual machines. For the VMs communications on the hypervisor I will be using Linux bridges. Other tools that we need for the VMs management are libvirt, virt-install or virt-manager which provides a gui.</p>

<p>Installing required packages for VMs management:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@lab:~# </span>aptitude install qemu-kvm qemu-utils libvirt-bin virtinst virt-manager bridge-utils</code></pre></figure>

<p>Now let’s create a bridge for each of the vms links plus an additional one used for out of band management. eth0 is my public interface so i will use it for the management bridge.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@lab:~# </span>brctl addbr mgmt
<span class="gp">root@lab:~# </span>brctl addif mgmt eth0
<span class="gp">root@lab:~# </span>brctl addbr vm1-sw1
<span class="gp">root@lab:~# </span>brctl addbr vm2-sw2
<span class="gp">root@lab:~# </span>brctl addbr sw1-sw2
<span class="gp">root@lab:~# </span>brctl addbr sw1-st1
<span class="gp">root@lab:~# </span>brctl addbr sw1-st2
<span class="gp">root@lab:~# </span>brctl addbr sw2-st2
<span class="gp">root@lab:~# </span>brctl addbr sw2-st1
<span class="gp">root@lab:~# </span>brctl show</code></pre></figure>

<p>Next thing to do is to create the virtual drives of the vms. I will use qcow2 files as they provide copy-on-write support and snapshots. You can use the qemu-img tools for creating the files. Each of the vms will be assigned one 10GB virtual drive for the OS. The storage servers will have 8 additional 260TB disks used for storage.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@lab:/var/lib/libvirt/images# </span>qemu-img create -f qcow2 vm1.qcow2 10G
<span class="gp">root@lab:/var/lib/libvirt/images# </span>qemu-img info vm1.qcow2
image: file
file format: qcow2
virtual size: 10G <span class="o">(</span>10737418240 bytes<span class="o">)</span>
disk size: 136K
cluster_size: 65536
<span class="gp">root@lab:/var/lib/libvirt/images# </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..8<span class="o">}</span>; <span class="k">do </span>qemu-img create -f qcow2 st2.<span class="nv">$i</span>.qcow2 260T;done</code></pre></figure>

<p>Now we are ready to shoot the installers. I prefer virt-install which is a CLI tool used for vms installations. You can also use virt-manager which provides a nice GUI. I will do a net install for the Debian VMs and run the installer from an attached cdrom for FreeBSD. You can use different drivers for the I/O devices. For the Linux machines I will use paravirtualized virtio drivers since they offer better performance. I noticed that FreeBSD doesn’t include native virtio support so I will use scsi drives and intel e1000 nics for it. You can even select the cpu model and what cpu features are available to the vm. The extra-args option passes the priority=low to the kernel command line which gets the Debian installer into expert mode.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@lab:~# </span>virt-install --name vm1 --disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/vm1.qcow2,bus<span class="o">=</span>virtio,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2 --network <span class="nv">bridge</span><span class="o">=</span>mgmt,model<span class="o">=</span>virtio --network <span class="nv">bridge</span><span class="o">=</span>vm1-sw1,model<span class="o">=</span>virtio --location<span class="o">=</span>ftp://ftp.lug.ro/debian/dists/wheezy/main/installer-amd64/ --ram<span class="o">=</span>512 --cpu<span class="o">=</span>Nehalem --vcpu<span class="o">=</span>1 --extra-args<span class="o">=</span><span class="s2">"priority=low"</span> --vnc</code></pre></figure>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@lab:~# </span>virt-install --name sw1 --disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/sw1.qcow2,bus<span class="o">=</span>virtio,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2 --network <span class="nv">bridge</span><span class="o">=</span>mgmt,model<span class="o">=</span>virtio --network <span class="nv">bridge</span><span class="o">=</span>vm1-sw1,model<span class="o">=</span>virtio --network <span class="nv">bridge</span><span class="o">=</span>sw1-st1,model<span class="o">=</span>virtio --network <span class="nv">bridge</span><span class="o">=</span>sw1-st2,model<span class="o">=</span>virtio --network <span class="nv">bridge</span><span class="o">=</span>sw1-sw2,model<span class="o">=</span>virtio --location<span class="o">=</span>ftp://ftp.lug.ro/debian/dists/wheezy/main/installer-amd64/ --ram<span class="o">=</span>512 --cpu<span class="o">=</span>Nehalem --vcpu<span class="o">=</span>1 --extra-args<span class="o">=</span><span class="s2">"priority=low"</span> --vnc</code></pre></figure>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@lab:~# </span>virt-install --name st2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.1.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.2.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.3.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.4.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.5.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.6.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.7.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--disk <span class="nv">path</span><span class="o">=</span>/var/lib/libvirt/images/st2.8.qcow2,bus<span class="o">=</span>scsi,cache<span class="o">=</span>none,format<span class="o">=</span>qcow2
--network <span class="nv">bridge</span><span class="o">=</span>mgmt,model<span class="o">=</span>e1000 --network <span class="nv">bridge</span><span class="o">=</span>sw2-st2,model<span class="o">=</span>e1000 --network <span class="nv">bridge</span><span class="o">=</span>sw2-st1,model<span class="o">=</span>e1000  --ram<span class="o">=</span>8192 --cpu<span class="o">=</span>Nehalem --vcpu<span class="o">=</span>6 --cdrom<span class="o">=</span>/var/lib/libvirt/images/FreeBSD-9.1-RELEASE-amd64-disc1.iso --vnc</code></pre></figure>

<p>The installer should be pretty straight forward. Once they are complete we can move forward. We’ll first need to set the IP addresses for the network interfaces and install the required packages. Please note that you need to edit /etc/network/interfaces to make the IP addresses persistent. 
Quick tip: when you debug network connectivity issues always check the link status of the interfaces ( physical, virtual, bridges, etc) and get tcpdump installed on the same line with your favorite text editor.</p>

<p>NFS Clients:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@vm1:~# </span>ip addr add 192.168.1.1/24 dev eth1
<span class="gp">root@vm1:~# </span>ip link <span class="nb">set </span>dev eth1 up
<span class="gp">root@vm1:~# </span>aptitude install nfs-common portmap</code></pre></figure>

<p>Now lets’s configure the switches/bridges. One important thing we need to take care of is the STP root bridge of our topology. We need to make sure that one of the switches will be the root bridge otherwise all the traffic will be forwarded by the storage servers bridges and it’s not their job to do that.</p>

<p>Open vSwitch:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@sw1:~# </span>aptitude install openvswitch-switch openvswitch-common
<span class="gp">root@sw1:~# </span>ovs-vsctl add-br sw1
<span class="gp">root@sw1:~# </span><span class="k">for </span>i <span class="k">in</span> <span class="o">{</span>1..4<span class="o">}</span>;<span class="k">do </span>ovs-vsctl add-port sw1 eth<span class="nv">$i</span>;<span class="k">done</span>;
<span class="gp">root@sw1:~# </span>ovs-vsctl show
8910e761-e507-4aa7-88b5-4c26e2e2276e
    Bridge <span class="s2">"sw1"</span>
        Port <span class="s2">"eth4"</span>
            Interface <span class="s2">"eth4"</span>
        Port <span class="s2">"sw1"</span>
            Interface <span class="s2">"sw1"</span>
                <span class="nb">type</span>: internal
        Port <span class="s2">"eth1"</span>
            Interface <span class="s2">"eth1"</span>
        Port <span class="s2">"eth2"</span>
            Interface <span class="s2">"eth2"</span>
        Port <span class="s2">"eth3"</span>
            Interface <span class="s2">"eth3"</span>
    ovs_version: <span class="s2">"1.4.2"</span>
<span class="gp">root@sw1:~# </span>ovs-vsctl <span class="nb">set </span>bridge sw1 <span class="nv">stp_enable</span><span class="o">=</span><span class="nb">true
</span><span class="gp">root@sw1:~# </span>ovs-vsctl <span class="nb">set </span>bridge sw1 other_config:stp-priority<span class="o">=</span>0x7800
<span class="gp">root@sw1:~# </span>ovs-ofctl show sw1
OFPT_FEATURES_REPLY <span class="o">(</span><span class="nv">xid</span><span class="o">=</span>0x1<span class="o">)</span>: ver:0x1, dpid:0000e22d31067b47
n_tables:255, n_buffers:256
features: capabilities:0xc7, actions:0xfff
 1<span class="o">(</span>eth2<span class="o">)</span>: addr:52:54:00:b0:d7:e5
     config:     0
     state:      STP_FORWARD
 2<span class="o">(</span>eth2<span class="o">)</span>: addr:52:54:00:79:59:68
     config:     0
     state:      STP_FORWARD
 3<span class="o">(</span>eth3<span class="o">)</span>: addr:52:54:00:69:dc:5d
     config:     0
     state:      STP_FORWARD
 4<span class="o">(</span>eth4<span class="o">)</span>: addr:52:54:00:e1:4d:dd
     config:     0
     state:      STP_FORWARD
 LOCAL<span class="o">(</span>sw1<span class="o">)</span>: addr:e2:2d:31:06:7b:47
     config:     PORT_DOWN
     state:      LINK_DOWN
OFPT_GET_CONFIG_REPLY <span class="o">(</span><span class="nv">xid</span><span class="o">=</span>0x3<span class="o">)</span>: <span class="nv">frags</span><span class="o">=</span>normal <span class="nv">miss_send_len</span><span class="o">=</span>0</code></pre></figure>

<p>Storage machines networking:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:~# </span>ifconfig bridge create
<span class="gp">root@st1:~# </span>ifconfig bridge0 addm em1 addm em2 up
<span class="gp">root@st1:~# </span>ifconfig bridge0 proto stp
<span class="gp">root@st1:~# </span>ifconfig bridge0 192.168.1.3 netmask 255.255.255.0
<span class="gp">root@st1:~# </span>ifconfig bridge0
bridge0: <span class="nv">flags</span><span class="o">=</span>8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500
	ether 02:d8:a2:2f:a0:00
	inet 192.168.1.3 netmask 0xffffff00 broadcast 192.168.1.255
	nd6 <span class="nv">options</span><span class="o">=</span>29&lt;PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL&gt;
	id 52:54:00:04:1b:42 priority 32768 hellotime 2 fwddelay 15
	maxage 20 holdcnt 6 proto stp maxaddr 2000 timeout 1200
	root id 56:ff:97:ea:f6:4b priority 30617 ifcost 20019 port 3
	member: em2 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 4 priority 128 path cost 20000 proto stp
	        role root state forwarding
	member: em1 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 3 priority 128 path cost 20000 proto stp
	        role alternate state discarding</code></pre></figure>

<p>We can notice that one of the physical interfaces is in forwarding state and the other one is discarding. The same settings need to be done on the other machines except IP addresses, hostnames and other things that need to be unique. At this point we have the network infrastructure ready with basic connectivity ensured for all the nodes.</p>

<p>We can go ahead and do the storage configuration. As mentioned before storage1 will be the primary storage machine so we’ll start with it.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:~# </span>camcontrol devlist
&lt;QEMU QEMU DVD-ROM 1.1.&gt;           at scbus1 target 0 lun 0 <span class="o">(</span>pass0,cd0<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 0 lun 0 <span class="o">(</span>pass1,da0<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 1 lun 0 <span class="o">(</span>pass2,da1<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 2 lun 0 <span class="o">(</span>pass3,da2<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 3 lun 0 <span class="o">(</span>pass4,da3<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 4 lun 0 <span class="o">(</span>pass5,da4<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 5 lun 0 <span class="o">(</span>pass6,da5<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus2 target 6 lun 0 <span class="o">(</span>pass7,da6<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus3 target 0 lun 0 <span class="o">(</span>pass8,da7<span class="o">)</span>
&lt;QEMU QEMU HARDDISK 1.1.&gt;          at scbus3 target 1 lun 0 <span class="o">(</span>pass9,da8<span class="o">)</span></code></pre></figure>

<p>The list shows us that we have one dvdrom (installer) and 9 drives (1 for the OS and 8 for the storage) attached.</p>

<p>ZFS:
Let’s create the ZFS storage pool. The pool will be made up of 4 mirrors of 2 x 260TB drives. ZFS mirrors are similar to RAID1 level. All the mirrors are striped so the resulting pool will have a RAID10 like fault tolerance. The total capacity of the pool will be of 1 Petabyte :)</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:~# </span>zpool create storage mirror da1 da2 mirror da3 da4 mirror da5 da6 mirror da7 da8
<span class="gp">root@st1:~# </span>zpool list
NAME      SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
storage  1.01P   110M  1.01P     0%  1.00x  ONLINE  -
<span class="gp">root@st1:~# </span>zpool status
  pool: storage
 state: ONLINE
  scan: none requested
config:
	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  mirror-0  ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0
	  mirror-1  ONLINE       0     0     0
	    da3     ONLINE       0     0     0
	    da4     ONLINE       0     0     0
	  mirror-2  ONLINE       0     0     0
	    da5     ONLINE       0     0     0
	    da6     ONLINE       0     0     0
	  mirror-3  ONLINE       0     0     0
	    da7     ONLINE       0     0     0
	    da8     ONLINE       0     0     0
errors: No known data errors
<span class="gp">root@st1:~# </span>zfs get all storage</code></pre></figure>

<p>We can now create a dataset inside the storage pool setting specific attributes. For instance, let’s create a dataset that has compression enabled:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:~# </span>zfs create -o <span class="nv">compression</span><span class="o">=</span>on storage/compset
<span class="gp">root@st1:~# </span>zfs get compression storage/compset
NAME             PROPERTY     VALUE     SOURCE
storage/compset  compression  on        <span class="nb">local</span></code></pre></figure>

<p>Now let’s ctually see the benefits:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:~# </span><span class="nb">cd</span> /storage/compset/
<span class="gp">root@st1:compset# </span>dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>10M <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span>10
10+0 records <span class="k">in
</span>10+0 records out
10485760 bytes transferred <span class="k">in </span>0.014196 secs <span class="o">(</span>738633678 bytes/sec
<span class="gp">root@st1:compset# </span>du -h 10M
512B	 10M
<span class="gp">root@st1:compset# </span>dd <span class="k">if</span><span class="o">=</span>/dev/random <span class="nv">of</span><span class="o">=</span>10Mr <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span>10
10+0 records <span class="k">in
</span>10+0 records out
10485760 bytes transferred <span class="k">in </span>0.176115 secs <span class="o">(</span>59539266 bytes/sec<span class="o">)</span>
<span class="gp">root@st1:compset# </span>du -h 10Mr
 10M	 10Mr</code></pre></figure>

<p>Let’s say we need storage space for users. We can create a dataset for that and assign each user a dataset that has an assigned quota:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:storage# </span>zfs create storage/users
<span class="gp">root@st1:storage# </span>zfs create -o <span class="nv">quota</span><span class="o">=</span>10M storage/users/01
<span class="gp">root@st1:storage# </span><span class="nb">cd</span> /storage/users/01
<span class="gp">root@st1:01# </span>dd <span class="k">if</span><span class="o">=</span>/dev/random <span class="nv">of</span><span class="o">=</span>20M <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span>20
dd: 20M: Disc quota exceeded
<span class="gp">root@st1:01# </span>du -h 20M
 10M	20M</code></pre></figure>

<p>Sanpshots:
ZFS provides snapshot functionality which is an image of the file system at the time you snapshot it. Let’s create a new data set called set, create some random files inside it and then snapshot it calling it snap1.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:~# </span>zfs create storage/set
<span class="gp">root@st1:~# </span><span class="nb">cd</span> /storage/set/
<span class="gp">root@st1:set# </span>mkfile 1M 1M
<span class="gp">root@st1:set# </span>mkfile 5M 5M
<span class="gp">root@st1:set# </span>zfs snapshot storage/set@snap1
<span class="gp">root@st1:set# </span>ls .zfs/snapshot/snap1/
1M 5M</code></pre></figure>

<p>How can we replicate all the data to the other storage machine ? ZFS enables you to transfer snapshots to another machine by piping zfs send and zfs recive which is pretty awesome. The receiving end will extract the snapshot and recreate the file system.</p>

<p>We have an empty storage pool called storage on storage2 where we’ll transfer snap1 we have just created. We’ll use netcat for raw tcp piping but you can also use ssh.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st2:~# </span>netcat -l -p 9999 | zfs receive storage/set
<span class="gp">root@st1:~# </span>zfs send storage/set@snap1 | netcat st2 9999
<span class="gp">root@st2:~# </span>zfs list
NAME          USED  AVAIL  REFER  MOUNTPOINT
storage      6.14M   508T    32K  /storage
storage/set  6.03M   508T  6.03M  /storage/set
<span class="gp">root@st2:~# </span>ls /storage/set/
1M 5M</code></pre></figure>

<p>You can also send incremental data using zfs send. We’ll create an additional file in /storage/set, create a snapshot and transfer only the differences between the second and the first snapshot. We’ll use ssh this time:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:set# </span>mkfile 10M 10M
<span class="gp">root@st1:set# </span>zfs snapshot storage/set@snap2
<span class="gp">root@st1:set# </span>zfs send -i storage/set@snap1 storage/set@snap2  | ssh st2 zfs receive storage/set
<span class="gp">root@st2:~# </span>ls /storage/set/
10M 1M  5M</code></pre></figure>

<p>What happens if we delete a file ?</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:set# </span>rm <span class="k">*</span> <span class="o">&amp;&amp;</span> touch Lala
<span class="gp">root@st1:set# </span>zfs snapshot storage/set@snap3
<span class="gp">root@st1:set# </span>zfs send -i storage/set@snap2 storage/set@snap3  | ssh st2 zfs receive storage/set
<span class="gp">root@st2:~# </span>ls /storage/set/
Lala</code></pre></figure>

<p>Using the great snapshot and zfs send and receive features we could easily set up a cron job that does periodic incremental transfers thus having all the data on storage1 replicated on storage2. 
Now that we have all the storage prepared let’s mount it on the clients by NFS. On the storage server prepare the /etc/exports file:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@st1:set# </span>cat /etc/exports
/storage/set -maproot<span class="o">=</span>root 192.168.1.1
<span class="gp">root@st1:set# </span>/etc/rc.d/mountd onereload
<span class="gp">root@vm1:~# </span>mount st1:/storage/set /mnt -v
<span class="gp">root@vm1:~# </span>df -h | grep st1
Filesystem                                              Size  Used Avail Use% Mounted on
st1:/storage/set                                       1016T     0 1016T   0% /mnt</code></pre></figure>

<p>Now let’s go for a failover test of the bridged interfaces on the storage machine. We’ll start writing a random file using dd in vm1:/mnt, shutdown the forwarding interface on storage1 and wait to see what happens.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@vm1:~# </span>dd <span class="k">if</span><span class="o">=</span>/dev/vda <span class="nv">of</span><span class="o">=</span>/root/random <span class="nv">bs</span><span class="o">=</span>1M <span class="nv">count</span><span class="o">=</span>10
<span class="gp">root@vm1:~# </span>md5sum /root/random
9662731c837501f21938b04e09aa02b3  random
<span class="gp">root@vm1:~# </span>dd <span class="k">if</span><span class="o">=</span>/root/random <span class="nv">of</span><span class="o">=</span>/mnt/random
<span class="gp">root@st1:set# </span>ifconfig bridge0
bridge0: <span class="nv">flags</span><span class="o">=</span>8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500
	ether 02:d8:a2:2f:a0:00
	inet 192.168.1.3 netmask 0xffffff00 broadcast 192.168.1.255
	nd6 <span class="nv">options</span><span class="o">=</span>29&lt;PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL&gt;
	id 52:54:00:04:1b:42 priority 32768 hellotime 2 fwddelay 15
	maxage 20 holdcnt 6 proto stp maxaddr 2000 timeout 1200
	root id 56:ff:97:ea:f6:4b priority 30617 ifcost 20000 port 4
	member: em2 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 4 priority 128 path cost 20000 proto stp
	        role root state forwarding
	member: em1 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 3 priority 128 path cost 20000 proto stp
	        role alternate state discarding
<span class="gp">root@st1:set# </span>ifconfig em2 down
<span class="gp">root@st1:set# </span>ifconfig bridge0
bridge0: <span class="nv">flags</span><span class="o">=</span>8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500
	ether 02:d8:a2:2f:a0:00
	inet 192.168.1.3 netmask 0xffffff00 broadcast 192.168.1.255
	nd6 <span class="nv">options</span><span class="o">=</span>29&lt;PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL&gt;
	id 52:54:00:04:1b:42 priority 32768 hellotime 2 fwddelay 15
	maxage 20 holdcnt 6 proto stp maxaddr 2000 timeout 1200
	root id 56:ff:97:ea:f6:4b priority 30617 ifcost 20019 port 3
	member: em2 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 4 priority 128 path cost 20000 proto stp
	        role disabled state discarding
	member: em1 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 3 priority 128 path cost 20000 proto stp
	        role root state learning
<span class="gp">root@st1:set# </span>ifconfig bridge0
bridge0: <span class="nv">flags</span><span class="o">=</span>8843&lt;UP,BROADCAST,RUNNING,SIMPLEX,MULTICAST&gt; metric 0 mtu 1500
	ether 02:d8:a2:2f:a0:00
	inet 192.168.1.3 netmask 0xffffff00 broadcast 192.168.1.255
	nd6 <span class="nv">options</span><span class="o">=</span>29&lt;PERFORMNUD,IFDISABLED,AUTO_LINKLOCAL&gt;
	id 52:54:00:04:1b:42 priority 32768 hellotime 2 fwddelay 15
	maxage 20 holdcnt 6 proto stp maxaddr 2000 timeout 1200
	root id 56:ff:97:ea:f6:4b priority 30617 ifcost 20019 port 3
	member: em2 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 4 priority 128 path cost 20000 proto stp
	        role disabled state discarding
	member: em1 <span class="nv">flags</span><span class="o">=</span>1c7&lt;LEARNING,DISCOVER,STP,AUTOEDGE,PTP,AUTOPTP&gt;
	        ifmaxaddr 0 port 3 priority 128 path cost 20000 proto stp
	        role root state forwarding</code></pre></figure>

<p>The result:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="gp">root@vm1:~# </span>dd <span class="k">if</span><span class="o">=</span>/root/random <span class="nv">of</span><span class="o">=</span>/mnt/random
10+0 records <span class="k">in
</span>10+0 records out
10485760 bytes <span class="o">(</span>10 MB<span class="o">)</span> copied, 56.1668 s, 187 kB/s
<span class="gp">root@st1:set# </span>md5 random
MD5 <span class="o">(</span>random<span class="o">)</span> <span class="o">=</span> 9662731c837501f21938b04e09aa02b</code></pre></figure>

<p>Transfer was very slow but your data is uncorrupted on the remote storage. 
I hope you enjoyed this long tutorial :) Let me know if you any questions or other observations and I’ll be more than happy to answer.</p>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/ansible-lemp-ha-cluster">
            LEMP stack HA cluster deployment with Ansible
            <small>26 Mar 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/rdo-manager-ha-openstack-deployment">
            HA Openstack deployment using RDO-Manager
            <small>17 Oct 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/raspberry-pi-car-automation">
            Graphing car metrics into the cloud with Raspberry Pi, OBD and Graphite
            <small>19 Apr 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
